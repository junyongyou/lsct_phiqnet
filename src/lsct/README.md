# LSCT Implementation

TF-Keras implementation of LSCT as described in [Long Short-term Convolutional Transformer for No-Reference Video Quality Assessment].

## Installation

1) Clone this repository.
2) Install required Python packages. The code is developed by PyCharm in Python 3.7. The requirements.txt document is generated by PyCharm, and the code should also be run in latest versions of the packages.

## Training a model
Examples of training LSCT and its variants can be seen in lsct/bin.
Argparser should be used, but the authors prefer to use dictionary with parameters being defined. It is easy to convert to take arguments.
In principle, the following parameters can be defined:

    args = {}
    args['multi_gpu'] = 0 # gpu setting, set to 1 for using multiple GPUs
    args['gpu'] = 0  # If having multiple GPUs, specify which GPU to use

    args['result_folder'] = r'..\databases\experiments' # Define result path
    args['vids_meta'] = r'..\\meta_data\all_vids.pkl'
    args['meta_file'] = r'..\\meta_data\all_video_mos.csv'
    
    # if ugc_chunk_pickle is used, then the folders containing PHIQNet features of UGC chunks must be specified
    args['ugc_chunk_pickle'] = r'..\\meta_data\ugc_chunks.pkl'  # this file contains information about the YouTube-UGC chunks, if set to None, then chunks are not included in training data
    args['ugc_chunk_folder'] = r'.\frame_features\ugc_chunks' # folder contains OHIQNet features on chunk frames, if ugc_chunk_pickle=None, then this argument is not used
    args['ugc_chunk_folder_flipped'] = r'.\frame_features_flipped\ugc_chunks' # folder contains OHIQNet features on chunk flipped frames, if ugc_chunk_pickle=None, then this argument is not used
    
    args['database'] = ['live', 'konvid', 'ugc'] # specify which database will be included in the training data
    
    args['model_name'] = 'lsct' # model name to be used in recording training result (e.g., logs) 
    
    args['transformer_params'] = [2, 64, 4, 64]
    args['dropout_rate'] = 0.1
    args['cnn_filters'] = [32, 64]
    # args['pooling_sizes'] = [4, 4]
    args['clip_length'] = 16
    
    args['lr_base'] = 1e-3  # Define the back learning rate in warmup and rate decay approach
    args['batch_size'] = 32 # Batch size, should choose to fit in the GPU memory
    args['epochs'] = 120  # Maximal epoch number, can set early stop in the callback or not
    args['lr_schedule'] = True  # Choose between True and False, indicating if learning rate schedule should be used or not
 
    args['validation'] = 'validation' # Choose between 'validation' and 'test'. If 'validation', the model will be trained on train set and validated on test set, which are randomly split from the databases. 
                                      # If 'test', the model will be trained on entire 'KonViD-1k' and 'YouTube-UGC' databases, and validated on the entire 'LIVE-VQC' database

    args['do_finetune'] = False # specify if finetune using SGD with smaller learning rate is performed

## Predict video quality using the trained model
After LSCT has been trained, and the weights have been stored in h5 file, it can be used to predict video quality with arbitrary resolutions.
In the "examples" folder, an example script examples\video_quality_prediction.py can predict quality of example video using the pretrained weights.

In order to predict video quality, both the PHIQNet weights and LSCT weights are required, and also FFMPEG (including FFProbe) are also needed to read video frames.
The pretrained weights of PHIQNet and LSCT can be found in model_weights folder.

## Prepare datasets for model training
This work uses three publicly available databases: KonViD-1k [The Konstanz natural video database (KoNViD-1k)](https://ieeexplore.ieee.org/document/7965673) by V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Sziranyi, S. Li, D. Saupe;
 YouTube-UGC [YouTube UGC dataset for video compression research](https://ieeexplore.ieee.org/document/8901772) by Y. Wang, S. Inguva, and B. Adsumilli;
 and LIVE-VQC [Large-scale study of perceptual video quality](https://ieeexplore.ieee.org/document/8463581) by Z. Sinno, and A.C. Bovik

1) The three databases can be used individually and also merged, and then randomly split to training and testing sets. 

2) Calculate PHIQNet features on all video frames. A script lsct\utils\frame_features_video_folders.py can be used to calculate PHIQNet features in a list of video folders. 
    An example script examples\frame_features_video.py also shows how to calculate PHIQNet features on video frames.
    Please download the PHIQNet weights file [here](https://drive.google.com/file/d/1ymy2oL0r-XNzjqk_kE-lcNkI2FhSu95h/view?usp=sharing), and store in in model_weights.
        
3) The frame features are better to stored in Numpy NPY files in target folders in lsct\utils\frame_features_video_folders.py. It is noted that flipped frames are also used for data augmentation. The frame features are in default stored in target_folder\frame_features, and the flipped features are stored in target_folder\frame_features_flipped.
  
4) Make meta file containing feature file paths and the MOS value, an example file is provided in lsct\meta_data\all_video_mos.csv:
    ```
        C:\vq_datasets\frame_features\live_vqc\Video\A001.npy,4.20928
        C:\vq_datasets\frame_features\live_vqc\Video\A002.npy,3.29202
        C:\vq_datasets\frame_features\live_vqc\Video\A004.npy,3.372716
        C:\vq_datasets\frame_features\live_vqc\Video\A005.npy,2.887112
        C:\vq_datasets\frame_features\live_vqc\Video\A006.npy,4.386068
        C:\vq_datasets\frame_features\live_vqc\Video\A007.npy,3.0347
    ```
   If the features are stored in other folders, please update this file. This file contains only frame features without flipping, and the script assumes that the flipped features can be accessed by replacing 'frame_features' by 'frame_features_flipped' in the meta file. So please store the features of flipped frames in that way.
   
5) Make a dumped pickle file containing a list of video IDs, which can be easily used to localize train and test videos. This can be done by lsct\utils\gather_video_ids.py.
    Video ID is formated as: database_video, e.g., live_A001.
    An example file is provided in lsct\meta_data\all_vids.pkl.  
    
6) If using YouTube-UGC chunks, then extract the PHIQNet features for individual trunks. This can be done by lsct\utils\ugc_chunk_generator.py, in which ugc_chunks.pkl can also be dumped.
    ugc_chunks.pkl contains a dictionary of: {UGC video name: [full MOS, chunk0 MOS, chunk1 MOS, ...]}.
    
7) The meta files together with the paths to chunk features (if used) should be provided for training, see lsct\bin\train_lsct_all_databases.py for an example.

## State-of-the-art models
Other NR-VQA models are also included in the work. The original implementations of metrics are employed, and they can be found below.

VBLLIDS: paper [Blind prediction of natural video quality](https://ieeexplore.ieee.org/document/6705673) by M. A. Saad, A. C. Bovik, and C. Charrierk, and [implementation](http://live.ece.utexas.edu/research/Quality/VideoBLIINDS_Code_MicheleSaad.zip).

ST-3DDCT: paper [Spatiotemporal statistics for video quality assessment](https://ieeexplore.ieee.org/document/7469872) by X. Li, Qun Guo, and Xiaoqiang Lu, and [implementation](https://github.com/scikit-video/scikit-video/tree/master/skvideo/measure).

TLVQM: paper [Two-level approach for no-reference consumer video quality assessment](https://ieeexplore.ieee.org/document/8742797) by J. Korhone, and [implementation](https://github.com/jarikorhonen/nr-vqa-consumervideo).

VSFA: paper [Quality assessment of in-the-wild videos](https://dl.acm.org/doi/10.1145/3343031.3351028) by D. Li, T. Jiang, and M. Jiang, and [implementation](https://github.com/lidq92/VSFA).

3D-CNN-LSTM: paper [Deep neural networks for no-reference video quality assessment](https://ieeexplore.ieee.org/document/8803395) by J. You, and J. Korhonen. 

VIDEAL: paper [UGC-VQA: Benchmarking blind video quality assessment for user generated content](https://arxiv.org/abs/2005.14354) by Z. Tu, Y. Wang, N. Birkbeck, B. Adsumilli, and A. C. Bovik, and [implementation](https://github.com/tu184044109/VIDEVAL_release).

## FAQ
* To be added
